{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77ee303",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fralfaro/CodingDojo-DataScience/blob/main/docs/8-Boosting/8-Boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Boosting\n",
    "\n",
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9ced8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Al igual que Bagging, el **Boosting** se encuentra en la categoría de métodos de **aprendizaje ensamblado**.\n",
    "- Bagging entrena un mismo modelo con **diferentes subsets de datos de entrenamiento**. (ej. Random Forest)\n",
    "- Boosting son aquellos métodos enfocados en **combinar** múltiples modelos débiles en un único modelo más robusto.\n",
    "- Otra diferencia radica en que bagging realiza entrenamientos de sus modelos en **paralelo**, mientras que boosting entrena sus modelos de manera **secuencial**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f16c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Bagging**\n",
    "<img src=\"../images/bagging.png\" alt=\"\" width=\"500px\"/>\n",
    "\n",
    "**Boosting**\n",
    "<img src=\"../images/boosting.png\" alt=\"\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddeb3db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## AdaBoost (Adaptive Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe792b",
   "metadata": {},
   "source": [
    "- Cada predictor corrige al predictor antecesor poniendo mayor atención a las instancias mal clasificadas.\n",
    "- AdaBoost incrementa el peso relativo de los datos de entrenamiento mal clasificados por el predictor anterior (mayor importancia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f548a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre processing\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer_data = load_breast_cancer(as_frame=True) \n",
    "\n",
    "X = cancer_data.data\n",
    "y = cancer_data.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=cancer_data.feature_names)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e752237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9766081871345029\n",
      "Train score 0.9849246231155779\n",
      "Test Target: [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "Test prediction [1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "adb = AdaBoostClassifier(LogisticRegression(), n_estimators=100, random_state=42)\n",
    "\n",
    "adb.fit(X_train, y_train)\n",
    "print('Test score:', adb.score(X_test, y_test))\n",
    "print('Train score', adb.score(X_train, y_train))\n",
    "print('Test Target:', y_test.values)\n",
    "print('Test prediction', adb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bbbc02",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7caf4a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A diferencia de AdaBoost, Gradient Boosting permite que cada predictor se ajuste a los **errores residuales** del predictor anterior.\n",
    "- Ojo: Si bien, teóricamente es posible usar otros modelos con Gradient Boosting, la biblioteca de Scikit-Learn solo permite ser utilizada con Árboles de Decisión como predictor base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6932ca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9590643274853801\n",
      "Test Target: [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "Test prediction [1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "print('Test score:', gb.score(X_test, y_test))\n",
    "print('Test Target:', y_test.values)\n",
    "print('Test prediction', gb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9c67a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## XGBoost vs LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b521c29",
   "metadata": {},
   "source": [
    "- La popularidad de Gradient Boosting ha generado múltiples implementaciones dirigidas a optimizar el cálculo de entrenamiento.\n",
    "- Dos implementaciones populares de Gradient Boosting en esta línea son **XGBoost** y **LightGBM**\n",
    "- **XGBoost** (eXtreme Gradient Boosting) es un algoritmo de boosting para árboles de decisión que aplica **crecimiento por nivel** (level-wise growth), el cual prioriza el crecimiento **horizontal** de los árboles\n",
    "- **LightGBM** (Light Gradient Boosting Machine) es un algoritmo de boosting para árboles de decisión que aplica **crecimiento por hojas** (leaf-wise tree growth), el cual prioriza el crecimiento **vertical** de los árboles.\n",
    "- Tanto XGBoost como LightGBM tienen sus **propias bibliotecas**, por lo que es necesario instalarlas previo uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6404f0e",
   "metadata": {},
   "source": [
    "### Crecimiento por nivel vs crecimiento por hojas\n",
    "<img src=\"../images/lightgb_xgboostm.png\" alt=\"\" width=\"900px\"/>\n",
    "<br>\n",
    "\n",
    "> **Fuente**: https://felipesulser.github.io/ashrae-kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5b5ec0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:16:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test score: 0.9824561403508771\n",
      "Test Target: [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "Test prediction [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "CPU times: user 1.32 s, sys: 0 ns, total: 1.32 s\n",
      "Wall time: 203 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polivares/anaconda3/envs/DataScience/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Esta sentencia permite visualizar el tiempo de ejecución de una celda\n",
    "# XGBoost (pip install xgboost)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "print('Test score:', xgb.score(X_test, y_test))\n",
    "print('Test Target:', y_test.values)\n",
    "print('Test prediction', xgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd92346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9590643274853801\n",
      "Test Target: [1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1]\n",
      "Test prediction [1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0\n",
      " 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n",
      "CPU times: user 3.31 s, sys: 0 ns, total: 3.31 s\n",
      "Wall time: 415 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LightGBM (pip install lightgbm)\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier(n_estimators=100, random_state=42)\n",
    "lgbm.fit(X_train, y_train)\n",
    "print('Test score:', lgbm.score(X_test, y_test))\n",
    "print('Test Target:', y_test.values)\n",
    "print('Test prediction', lgbm.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba661a96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Actividad 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847f5ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Utilice nuevamente el dataset California Housing presente en los dataset de Scikit-Learn (enlace [aquí](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)) y utilice los algoritmos de boosting para regresión de datos. \n",
    "- Compare los resultados obtenidos para cada algoritmo utilizando distintas métricas"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "8bb49ada1c49af35e8d68aa4f21a32c70a905f8c89742e7f0056ae28466ec816"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
